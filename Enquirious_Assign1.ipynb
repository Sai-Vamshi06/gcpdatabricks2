{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab7addf-befe-4039-90bb-7853687baf20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Mount\n",
    "# Set up the configurations for mounting the GCS bucket\n",
    "gcs_bucket_name = \"gbmecom\"\n",
    "mount_point = \"/mnt/mskltestmnt033\"\n",
    "project_id = \"mentorsko-1723044085161\"\n",
    "service_account_key = \"/dbfs/FileStore/tables/mentorsko_1723044085161_d092fa612a29.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7768d609-90c6-41b8-b1c9-bf54a5176dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the service account key file\n",
    "with open(service_account_key, 'r') as key_file:\n",
    "    service_account_info = json.load(key_file)\n",
    "\n",
    "# Define the GCS service account credentials\n",
    "config = {\n",
    "    \"fs.gs.impl\": \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\",\n",
    "    \"fs.gs.auth.service.account.enable\": \"true\",\n",
    "    \"fs.gs.auth.service.account.email\": service_account_info[\"client_email\"],\n",
    "    \"fs.gs.auth.service.account.private.key.id\": service_account_info[\"private_key_id\"],\n",
    "    \"fs.gs.auth.service.account.private.key\": service_account_info[\"private_key\"],\n",
    "    \"fs.gs.project.id\": project_id\n",
    "}\n",
    "\n",
    "# Mount the GCS bucket\n",
    "dbutils.fs.mount(\n",
    "    source=f\"gs://{gcs_bucket_name}\",\n",
    "    mount_point=mount_point,\n",
    "    extra_configs=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61cc5dfd-23a4-4b7c-a299-4621cad7641e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(mount_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18d7ce9-4dfc-4ddd-8559-bdf622b06dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"dbfs:/mnt/mskltestmnt033/returns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "49c5e1d6-1825-4eb0-9e7a-892a4e6b1020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e237a732-94d9-4b08-957c-7c4681601e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enquiriuos-1. Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3f5ff3-572d-4b83-8178-23abc89c7de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "addresses_path = 'dbfs:/FileStore/tables/addresses.csv'\n",
    "addresses_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(addresses_path)\n",
    "customers_path = 'dbfs:/FileStore/tables/customers.csv'\n",
    "customers_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(customers_path)\n",
    "orders_path = 'dbfs:/FileStore/tables/orders.csv'\n",
    "orders_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(orders_path)\n",
    "order_items_path = 'dbfs:/FileStore/tables/orders_items.csv'\n",
    "orders_items_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(order_items_path)\n",
    "products_path = 'dbfs:/FileStore/tables/products.csv'\n",
    "products_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(products_path)\n",
    "suppliers_path = 'dbfs:/FileStore/tables/suppliers.csv'\n",
    "suppliers_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(suppliers_path)\n",
    "# \n",
    "payments_path = 'dbfs:/FileStore/tables/payments.csv'\n",
    "payment_method_path = 'dbfs:/FileStore/tables/payment_methods.csv'\n",
    "payments_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(payments_path)\n",
    "payment_method_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(payment_method_path)\n",
    "shipping_path = 'dbfs:/FileStore/tables/shipping_tier.csv'\n",
    "shipping_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(shipping_path)\n",
    "returns_path = 'dbfs:/FileStore/tables/returns.csv'\n",
    "returns_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(returns_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "455eb9e8-1f0e-4dcd-ba6f-8a4aa05030a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# orders_df.display()\n",
    "# order_items_df.display()\n",
    "# payments_df.display()\n",
    "# payment_method_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4fb997f1-d7f0-4bfe-b7cb-30f601a6b13d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# The above line of code is to import the sql functions in pyspark.\n",
    "payment_method_preference = (\n",
    "    payments_df\n",
    "    .join(payment_method_df, on='PaymentMethodID')\n",
    "    .groupBy('MethodName')\n",
    "    .count()\n",
    "    .orderBy(col(\"count\").desc())\n",
    ")\n",
    "\n",
    "payment_method_preference.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf538b04-2cc4-41dc-b5a7-822f9e367da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"Order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "23a274f5-75f9-4380-8fb7-178a5a50ee53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select\n",
    "    ProductID,\n",
    "    sum(Quantity) as TotalQuantitySold\n",
    "from\n",
    "  order_items_df\n",
    "GROUP BY ProductID\n",
    "ORDER BY TotalQuantitySold DESC\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa35551-230c-4da6-9e50-1b524a505a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceGlobalTempView(\"orders\")\n",
    "orders_items_df.createOrReplaceGlobalTempView(\"orders_items\")\n",
    "payments_df.createOrReplaceGlobalTempView(\"payments\")\n",
    "payment_method_df.createOrReplaceGlobalTempView(\"payment_methods\")\n",
    "products_df.createOrReplaceGlobalTempView(\"products\")\n",
    "suppliers_df.createOrReplaceGlobalTempView(\"suppliers\")\n",
    "addresses_df.createOrReplaceGlobalTempView(\"addresses\")\n",
    "customers_df.createOrReplaceGlobalTempView(\"customers\")\n",
    "shipping_df.createOrReplaceGlobalTempView(\"shipping\")\n",
    "returns_df.createOrReplaceGlobalTempView(\"returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "12b83d58-9f79-4e92-b1dd-99821791a0cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    o.OrderChannel,\n",
    "    p.PaymentMethodID,\n",
    "    COUNT(o.OrderID) as NumberOfOrders\n",
    "FROM global_temp.orders o\n",
    "JOIN \n",
    "    global_temp.payments p on o.OrderID = p.OrderID\n",
    "GROUP BY o.OrderChannel, p.PaymentMethodID\n",
    "ORDER BY NumberOfOrders DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9e1a419-a2ad-4867-b382-a7353d289c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "023d49a6-b623-4bef-94d1-a51c28c0f76b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754983247527}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with t1 as (\n",
    "SELECT c.CustomerID, c.FirstName, p.Product_ID, p.Discounted_Price , oi.Quantity \n",
    "FROM global_temp.customers c\n",
    "JOIN global_temp.orders o on c.CustomerID = o.CustomerID\n",
    "JOIN global_temp.orders_items oi on o.OrderID = oi.OrderID\n",
    "JOIN global_temp.products p on oi.ProductID = p.Product_ID\n",
    ")\n",
    "select CustomerID,FirstName, sum(CAST(regexp_replace(discounted_price, '[^0-9]', '') AS INT) * Quantity) as ttl_sales\n",
    "from t1\n",
    "group by CustomerID,firstname\n",
    "order by ttl_sales desc\n",
    "limit 10\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e2090083-c474-4e4f-97f4-f8bf9e66a0b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT s.TierName, COUNT(o.OrderID) AS order_count\n",
    "FROM global_temp.orders o\n",
    "JOIN global_temp.shipping s ON o.ShippingTierID = s.ShippingTierID\n",
    "GROUP BY s.TierName\n",
    "ORDER BY order_count DESC\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "30d94404-1c89-4776-8a60-09717f67c653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generated = DBX_helper\n",
    "most_popular_shiptier = spark.sql(\"\"\"\n",
    "    SELECT s.TierName, COUNT(*) AS order_count\n",
    "    FROM global_temp.orders o\n",
    "    JOIN global_temp.shipping s\n",
    "      ON o.ShippingTierID = s.ShippingTierID\n",
    "    GROUP BY s.TierName\n",
    "    ORDER BY order_count DESC\n",
    "    -- LIMIT 1\n",
    "\"\"\")\n",
    "display(most_popular_shiptier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "999ba45d-1257-4469-9024-a3a073a91651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT p.Product_ID, p.Product_Name, count(o.orderID) as total_sales\n",
    "FROM global_temp.products p\n",
    "JOIN global_temp.orders_items o on o.ProductID = p.Product_ID\n",
    "GROUP BY p.Product_ID, p.Product_Name\n",
    "ORDER BY total_sales DESC\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43376d9-ea92-41dd-8c3a-91c634998687",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755060429309}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with t1 as (\n",
    "SELECT c.CustomerID, o.OrderID,(CAST(regexp_replace(p.discounted_price, '[^0-9]', '') AS INT) * oi.Quantity) as sales\n",
    "        ,oi.Quantity\n",
    "        , O.OrderDate, O.ActualDeliveryDate\n",
    "FROM global_temp.customers c\n",
    "JOIN global_temp.orders o on c.CustomerID = o.CustomerID\n",
    "JOIN global_temp.orders_items oi on o.OrderID = oi.OrderID\n",
    "JOIN global_temp.products p on oi.ProductID = p.Product_ID\n",
    ")\n",
    "select customerid\n",
    "        , count(OrderID) AS total_orders, sum(sales) as ttl_sales, avg(Quantity) as avg_bucket_size\n",
    "        , datediff(DAY, min(OrderDate),max(ActualDeliveryDate)) as length_of_stay_days\n",
    "from t1\n",
    "group by customerID\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1afca08-3bfb-492a-bcc7-353a941853ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d57375b8-7cc1-4ed3-9310-fd79d801fb0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lake House = Data Lake(cheap,can have any kind of data,not an ACID complaince,) + Data Warehouse(OLAP,storage, compute,making reports/analytics).\n",
    "\n",
    "Data bricks is storing the entire database in Data lake. So we use Delta lake, which supports ACID property-helpful.\n",
    "\n",
    "**Delta Lake** : \n",
    "Delta lake supports all ACID properties.\n",
    "parquet file format is the default format in delta lake.\n",
    "parquet is a columnar structured format whereas .csv is a row structured form of the data.\n",
    "if we apply SQL commands on a .csv file it will be processed in row-wise.It will take time for specific row related data.\n",
    "\n",
    "Delta lake usually creates versions of the data for updating and other tasks.\n",
    "Delta lake creates new parquet file everytime u change something in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b40d2987-dc4e-42cc-aabc-d7e27ad64ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Ways to build Bronze Layer**\n",
    "\n",
    "insert into ? -- does it append?\n",
    "insert override -- if destination has 500 records and destination has 20 records. It will override the 500 data as 20 data records and we will lose previous 500 records.\n",
    "\n",
    "So we use COPY INTO -- It's souce = cloud storage  && destination = Delta table.\n",
    "Copy Into command skips all the already processed data and processes only the new data maintaining the old records in the data itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e14f0f81-b113-488c-92d2-13fab0b85c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64c062c2-8128-4fc0-9312-ab12e9e66ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7419342635064603,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Enquirious_Assign1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
